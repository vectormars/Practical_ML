---
title: "Project_Course8"
author: "Jie Xue"
date: "June 1, 2016"
output: html_document
---
Clear the space
```{r 0}
rm(list=ls())
cat("\014")
```

# Part 1. Data
First, loading the training data/testing data with replacing all missing with "NA"
```{r 1}
Train<-read.csv("pml-training.csv",na.strings=c("NA","#DIV/0!",""))
Test<-read.csv("pml-testing.csv",na.strings=c("NA","#DIV/0!",""))     
```

Then, explore the data a little bit.
```{r 2}
dim(Train)
```

See, how classe distributed
```{r 3}
Train.Class<-Train$classe
classe.freq<-table(Train.Class)
classe.freq<-as.data.frame(classe.freq)
```

Bar plot of classe with ggplot
```{r 4}
library("ggplot2")
p<-ggplot(classe.freq,aes(x=Train.Class,y=Freq),fill=Train.Class)+
    geom_bar(stat="identity", fill="steelblue") +
    geom_text(aes(label=Freq), vjust=-0.3,size=3.5)+
    ylab("Classe Frequency") +
    xlab("Classe type") 
print(p)
```


# Part 2. Pre-Processing
Remove columns woth more than 50% NAs
```{r 21}
Train <- Train[, colSums(is.na(Train)) < nrow(Train) * 0.5]
Test <- Test[, colSums(is.na(Test)) < nrow(Test) * 0.5]

```


Remove all Near Zero Variance variables
```{r 22}
library(lattice)
library(caret)
NZV <- nearZeroVar(Train, saveMetrics= TRUE)
Train <- Train[,!NZV$nzv]
Test <- Test[,!NZV$nzv]
```


Remove unnecessary columns 1 to 6
```{r 23}
Train<-Train[,-c(1:6) ]
Test<-Test[,-c(1:6) ]
```


Partition data into 60% and 40%
```{r 24}
set.seed(123)
DTrain<-createDataPartition(Train$classe, p=0.7, list=FALSE)
Train.T<-Train[DTrain,]
Train.CV<-Train[-DTrain,]
```


# Part 3. Build prediction model
First, Try decision tree
```{r 31}
Model.DT<-train(classe ~ ., method="rpart",data=Train.T) 
Prediction.DT <- predict(Model.DT, Train.CV)
```
Test results on our subTesting data set:
```{r 32}
confusionMatrix(Prediction.DT, Train.CV$classe)
```
The results are not good enough. Try another algorithm.      


Second, try random forest
```{r 33}
library(randomForest)
Model.RF <- randomForest(classe~.,data=Train.T)
Prediction.RF <- predict(Model.RF, Train.CV)
confusionMatrix(Prediction.RF, Train.CV$classe)
```



Check the Importance with Overall>200
```{r 34}
importance <- varImp(Model.RF)
RN<-rownames(importance)
importance<-cbind(RN,importance)
library(dplyr)
importance<-arrange(importance,desc(Overall))
importance<-filter(importance,Overall>200)
p<-ggplot(importance,aes(x=RN,y=Overall),fill=Train.Class)+
    geom_bar(stat="identity", fill="steelblue") +
    geom_text(aes(label=Overall),hjust=0,size=3.5)+
    coord_flip()+
    ylab("Overall") +
    xlab("Important Features") 
print(p)

```







# part 4. Using the test data
```{r 41}
Prediction.Test <- predict(Model.RF, Test)
Prediction.Test
```








